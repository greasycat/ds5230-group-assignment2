year,title,authors,summary,category,reconstruction_error
2021,highly accurate protein structure prediction with alphafold,john jumper et al,proteins are essential to life and understanding their structure can facilitate a mechanistic understanding of their function through an enormous experimental effort12 34 the structures of around 100000 unique proteins have been determined5 but this represents a small fraction of the billions of known protein sequences67 structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure accurate computational approaches are needed to address this gap and to enable large scale structural bioinformatics predicting the three dimensional structure that a protein will adopt based solely on its amino acid sequence the structure prediction component of the protein folding problem 8 has been an important open research problem for more than 50 years9 despite recent progress1011121314 existing methods fall far short of atomic accuracy especially when no homologous structure is available here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known we validated an entirely redesigned version of our neural network based model alphafold in the challenging 14th critical assessment of protein structure prediction casp14 15 demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods underpinning the latest version of alphafold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure leveraging multi sequence alignments into the design of the deep learning algorithm,ml_general,0.02680522150484071
2016,hybrid computing using a neural network with dynamic external memory,graves a et al,artificial neural networks are remarkably adept at sensory processing sequence learning and reinforcement learning but are limited in their ability to represent variables and data structures and to store data over long timescales owing to the lack of an external memory here we introduce a machine learning model called a differentiable neural computer dnc which consists of a neural network that can read from and write to an external memory matrix analogous to the random access memory in a conventional computer like a conventional computer it can use its memory to represent and manipulate complex data structures but like a neural network it can learn to do so from data when trained with supervised learning we demonstrate that a dnc can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language we show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs and then generalize these tasks to specific graphs such as transport networks and family trees when trained with reinforcement learning a dnc can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols taken together our results demonstrate that dncs have the capacity to solve complex structured tasks that are inaccessible to neural networks without external read write memory,dl_rnn,0.028012122414083556
2021,an introduction to johnson lindenstrauss transforms,casper benjamin freksen,johnson lindenstrauss transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data and they have found use in many fields from machine learning to differential privacy and more this note explains what they are it gives an overview of their use and their development since they were introduced in the 1980s and it provides many references should the reader wish to explore these topics more deeply,ml_general,0.03666132690513847
2022,magnetic control of tokamak plasmas through deep reinforcement learning,jonas degrave et al,nuclear fusion using magnetic confinement in particular in the tokamak configuration is a promising path towards sustainable energy a core challenge is to shape and maintain a high temperature plasma within the tokamak vessel this requires high dimensional high frequency closed loop control using magnetic actuator coils further complicated by the diverse requirements across a wide range of plasma configurations in this work we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils this architecture meets control objectives specified at a high level at the same time satisfying physical and operational constraints this approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations we successfully produce and control a diverse set of plasma configurations on the tokamak à configuration variable12 including elongated conventional shapes as well as advanced configurations such as negative triangularity and snowflake configurations our approach achieves accurate tracking of the location current and shape for these configurations we also demonstrate sustained droplets on tcv in which two separate plasmas are maintained simultaneously within the vessel this represents a notable advance for tokamak feedback control showing the potential of reinforcement learning to accelerate research in the fusion domain and is one of the most challenging real world systems to which reinforcement learning has been applied,dl_rl,0.030390750256837556
2012,deep neural networks for acoustic modeling in speech recognition the shared views of four research groups,g hinton et al,most current speech recognition systems use hidden markov models hmms to deal with the temporal variability of speech and gaussian mixture models gmms to determine how well each state of each hmm fits a frame or a short window of frames of coefficients that represents the acoustic input an alternative way to evaluate the fit is to use a feed forward neural network that takes several frames of coefficients as input and produces posterior probabilities over hmm states as output deep neural networks dnns that have many hidden layers and are trained using new methods have been shown to outperform gmms on a variety of speech recognition benchmarks sometimes by a large margin this article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using dnns for acoustic modeling in speech recognition,audio,0.027483181371346255
1990,the strength of weak learnability,robert e schapire,this paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution free pac learning model a concept class islearnable orstrongly learnable if given access to a source of examples of the unknown concept the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances the concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing in this paper it is shown that these two notions of learnability are equivalent a method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy this construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well in addition the construction has some interesting theoretical consequences including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error ο,ml_general,0.025966735293310794
1991,bagging predictors,leo breiman,bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor the aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class the multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy the vital element is the instability of the prediction method if perturbing the learning set can cause significant changes in the predictor constructed then bagging can improve accuracy,ml_general,0.026906094424849897
2001,random forests,leo breiman,random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest the generalization error for forests converges a s to a limit as the number of trees in the forest becomes large the generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them using a random selection of features to split each node yields error rates that compare favorably to adaboost y freund r schapire machine learning proceedings of the thirteenth international conference 148 156 but are more robust with respect to noise internal estimates monitor error strength and correlation and these are used to show the response to increasing the number of features used in the splitting internal estimates are also used to measure variable importance these ideas are also applicable to regression,ml_general,0.035863550596820414
1998,efficient backprop,yann lecun et al,the convergence of back propagation learning is analyzed so as to explain common phenomenon observed by practitioners many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications this paper gives some of those tricks and offers explanations of why they work many authors have suggested that second order optimization methods are advantageous for neural net training it is shown that most classical second order methods are impractical for large neural networks a few methods are proposed that do not have these limitations,ml_general,0.02701028718395331
2003,document clustering based on non negative matrix factorization,wei xu et al,in this paper we propose a novel document clustering method based on the non negative factorization of the term document matrix of the given document corpus in the latent semantic space derived by the non negative matrix factorization nmf each axis captures the base topic of a particular document cluster and each document is represented as an additive combination of the base topics the cluster membership of each document can be easily determined by finding the base topic the axis with which the document has the largest projection value our experimental evaluations show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering methods not only in the easy and reliable derivation of document clustering results but also in document clustering accuracies,ml_general,0.03017299021666056
1991,recurrent networks and narma modeling,j connor et al,there exist large classes of time series such as those with nonlinear moving average components that are not well modeled by feedforward networks or linear models but can be modeled by recurrent networks we show that recurrent neural networks are a type of nonlinear autoregressive moving average narma model practical ability will be shown in the results of a competition sponsored by the puget sound power and light company where the recurrent networks gave the best performance on electric load forecasting,dl_rnn,0.025663294372013173
1996,constructing deterministic finite state automata in recurrent neural networks,omlin c w giles c l,recurrent neural networks that are trained to behave like deterministic finite state automata dfas can show deteriorating performance when tested on long strings this deteriorating performance can be attributed to the instability of the internal representation of the learned dfa states the use of a sigmoidel discriminant function together with the recurrent structure contribute to this instability we prove that a simple algorithm can construct second order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal dfa state representations are stable that is the constructed network correctly classifies strings of arbitrary length the algorithm is based on encoding strengths of weights directly into the neural network we derive a relationship between the weight strength and the number of dfa states for robust string classification for a dfa with n state and minput alphabet symbols the constructive algorithm generates a programmed neural network with o n neurons and o mn weights we compare our algorithm to other methods proposed in the literature,dl_rnn,0.02700859744189636
2023,3d gaussian splatting for real time radiance field rendering,bernhard kerbl et al,radiance field methods have recently revolutionized novel view synthesis of scenes captured with multiple photos or videos however achieving high visual quality still requires neural networks that are costly to train and render while recent faster methods inevitably trade off speed for quality for unbounded and complete scenes rather than isolated objects and 1080p resolution rendering no current method can achieve real time display rates we introduce three key elements that allow us to achieve state of the art visual quality while maintaining competitive training times and importantly allow high quality real time 30 fps novel view synthesis at 1080p resolution first starting from sparse points produced during camera calibration we represent the scene with 3d gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space second we perform interleaved optimization density control of the 3d gaussians notably optimizing anisotropic covariance to achieve an accurate representation of the scene third we develop a fast visibility aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering we demonstrate state of the art visual quality and real time rendering on several established datasets,cv_pattern,0.027626282981604838
2014,decaf a deep convolutional activation feature for generic visual recognition,j donahue et al,we evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large fixed set of object recognition tasks can be re purposed to novel generic tasks our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks we investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks including scene recognition domain adaptation and fine grained recognition challenges we compare the efficacy of relying on various network levels to define a fixed feature and report novel results that significantly outperform the state of the art on several important vision challenges we are releasing decaf an open source implementation of these deep convolutional activation features along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms,cv_pattern,0.025513316788065416
2013,distributed representations of words and phrases and their compositionality,tomas mikolov et al,the recently introduced continuous skip gram model is an efficient method for learning high quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships in this paper we present several extensions that improve both the quality of the vectors and the training speed by subsampling of the frequent words we obtain significant speedup and also learn more regular word representations we also describe a simple alternative to the hierarchical softmax called negative sampling an inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases for example the meanings of canada and air cannot be easily combined to obtain air canada motivated by this example we present a simple method for finding phrases in text and show that learning good vector representations for millions of phrases is possible,dl_nlp,0.02607025099974522
2015,deep unsupervised learning using nonequilibrium thermodynamics,jascha sohl dickstein et al,a central problem in machine learning involves modeling complex data sets using highly flexible families of probability distributions in which learning sampling inference and evaluation are still analytically or computationally tractable here we develop an approach that simultaneously achieves both flexibility and tractability the essential idea inspired by non equilibrium statistical physics is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process we then learn a reverse diffusion process that restores structure in data yielding a highly flexible and tractable generative model of the data this approach allows us to rapidly learn sample from and evaluate probabilities in deep generative models with thousands of layers or time steps as well as to compute conditional and posterior probabilities under the learned model we additionally release an open source reference implementation of the algorithm,ml_general,0.0282939301456057
2015,vqa visual question answering,s antol et al,we propose the task of free form and open ended visual question answering vqa given an image and a natural language question about the image the task is to provide an accurate natural language answer mirroring real world scenarios such as helping the visually impaired both the questions and answers are open ended visual questions selectively target different areas of an image including background details and underlying context as a result a system that succeeds at vqa typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions moreover vqa is amenable to automatic evaluation since many open ended answers contain only a few words or a closed set of answers that can be provided in a multiple choice format we provide a dataset containing 0 25m images 0 76m questions and 10m answers and discuss the information it provides numerous baselines and methods for vqa are provided and compared with human performance our vqa demo is available on cloudcv,cv_generative,0.025538626280106603
2017,simplified minimal gated unit variations for recurrent neural networks,heck j salem f m,recurrent neural networks with various types of hidden units have been used to solve a diverse range of problems involving sequence data two of the most recent proposals gated recurrent units gru and minimal gated units mgu have shown comparable promising results on example public datasets in this paper we introduce three model variants of the minimal gated unit mgu which further simplify that design by reducing the number of parameters in the forget gate dynamic equation these three model variants referred to simply as mgu1 mgu2 and mgu3 were tested on sequences generated from the mnist dataset and from the reuters newswire topics rnt dataset the new models have shown similar accuracy to the mgu model while using fewer parameters and thus lowering training expense one model variant namely mgu2 performed better than mgu on the datasets considered and thus may be used as an alternate to mgu or gru in recurrent neural networks,dl_rnn,0.027006971344462588
2020,implicit neural representations with periodic activation functions,vincent sitzmann et al,implicitly defined continuous differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm offering many possible benefits over conventional representations however current network architectures for such implicit neural representations are incapable of modeling signals with fine detail and fail to represent a signal s spatial and temporal derivatives despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations we propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks dubbed sinusoidal representation networks or sirens are ideally suited for representing complex natural signals and their derivatives we analyze siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images wavefields video sound and their derivatives further we show how sirens can be leveraged to solve challenging boundary value problems such as particular eikonal equations yielding signed distance functions the poisson equation and the helmholtz and wave equations lastly we combine sirens with hypernetworks to learn priors over the space of siren functions,ml_general,0.03085064657185953
2020,fourier features let networks learn high frequency functions in low dimensional domains,matthew tancik et al,we show that passing input points through a simple fourier feature mapping enables a multilayer perceptron mlp to learn high frequency functions in low dimensional problem domains these results shed light on recent advances in computer vision and graphics that achieve state of the art results by using mlps to represent complex 3d objects and scenes using tools from the neural tangent kernel ntk literature we show that a standard mlp fails to learn high frequencies both in theory and in practice to overcome this spectral bias we use a fourier feature mapping to transform the effective ntk into a stationary kernel with a tunable bandwidth we suggest an approach for selecting problem specific fourier features that greatly improves the performance of mlps for low dimensional regression tasks relevant to the computer vision and graphics communities,ml_general,0.026566608285768706
2021,learning in high dimension always amounts to extrapolation,randall balestriero et al,the notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset s convex hull extrapolation occurs when x falls outside of that convex hull one fundamental mis conception is that state of the art algorithms work so well because of their ability to correctly interpolate training data a second mis conception is that interpolation happens throughout tasks and datasets in fact many intuitions and theories rely on that assumption we empirically and theoretically argue against those two points and demonstrate that on any high dimensional 100 dataset interpolation almost surely never happens those results challenge the validity of our current interpolation extrapolation definition as an indicator of generalization performances,ml_general,0.03394608009063524
2022,learning robust perceptive locomotion for quadrupedal robots in the wild,joonho lee et al,legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into under explored areas exteroceptive perception is crucial for fast and energy efficient locomotion perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability however utilizing exteroceptive perception robustly for locomotion has remained a grand challenge in robotics snow vegetation and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance additionally depth perception can degrade due to difficult lighting dust fog reflective or transparent surfaces sensor occlusion and more for this reason the most robust and general solutions to legged locomotion to date rely solely on proprioception this severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly here we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion we leverage an attention based recurrent encoder that integrates proprioceptive and exteroceptive input the encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics the result is a legged locomotion controller with high robustness and speed the controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour long hike in the alps in the time recommended for human hikers,dl_rl,0.03666944547107864
2022,bc z zero shot task generalization with robotic imitation learning,eric jang et al,in this paper we study the problem of enabling a vision based robotic manipulation system to generalize to novel tasks a long standing challenge in robot learning we approach the challenge from an imitation learning perspective aiming to study how scaling and broadening the data collected can facilitate such generalization to that end we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task including pre trained embeddings of natural language or videos of humans performing the task when scaling data collection on a real robot to more than 100 distinct tasks we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44 without any robot demonstrations for those tasks,dl_rl,0.026803165422577945
2022,solving quantitative reasoning problems with language models,lewkowycz et al,language models have achieved remarkable performance on a wide range of tasks that require natural language understanding nevertheless state of the art models have generally struggled with tasks that require quantitative reasoning such as solving mathematics science and engineering problems at the college level to help close this gap we introduce minerva a large language model pretrained on general natural language data and further trained on technical content the model achieves state of the art performance on technical benchmarks without the use of external tools we also evaluate our model on over two hundred undergraduate level problems in physics biology chemistry economics and other sciences that require quantitative reasoning and find that the model can correctly answer nearly a third of them,dl_nlp,0.02658787267251679
2023,stabilizing transformer training by preventing attention entropy collapse,shuangfei zhai et al,training stability is of great importance to transformers in this work we investigate the training dynamics of transformers by examining the evolution of the attention layers in particular we track the attention entropy for each attention head during the course of training which is a proxy for model sharpness we identify a common pattern across different architectures and tasks where low attention entropy is accompanied by high training instability which can take the form of oscillating loss or divergence we denote the pathologically low attention entropy corresponding to highly concentrated attention scores as TEXTIT entropy collapse TEXTIT as a remedy we propose sigma reparam a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar we demonstrate that sigma reparam successfully prevents entropy collapse in the attention layers promoting more stable training additionally we prove a tight lower bound of the attention entropy which decreases exponentially fast with the spectral norm of the attention logits providing additional motivation for our approach we conduct experiments with sigma reparam on image classification image self supervised learning machine translation speech recognition and language modeling tasks we show that sigma reparam provides stability and robustness with respect to the choice of hyperparameters going so far as enabling training a a vision transformer to competitive performance without warmup weight decay layer normalization or adaptive optimizers b deep architectures in machine translation and c speech recognition to competitive performance without warmup and adaptive optimizers code is available at url,dl_nlp,0.03117310344110596
2023,neuralangelo high fidelity neural surface reconstruction,zhaoshuo li et al,neural surface reconstruction has been shown to be powerful for recovering dense 3d surfaces via image based neural rendering however current methods struggle to recover detailed structures of real world scenes to address the issue we present neuralangelo which combines the representation power of multi resolution 3d hash grids with neural surface rendering two key ingredients enable our approach 1 numerical gradients for computing higher order derivatives as a smoothing operation and 2 coarse to fine optimization on the hash grids controlling different levels of details even without auxiliary inputs such as depth neuralangelo can effectively recover dense 3d surface structures from multi view images with fidelity significantly surpassing previous methods enabling detailed large scale scene reconstruction from rgb video captures,cv_pattern,0.028400438363000814
1991,multivariate adaptive regression splines,jerome h friedman,a new method is presented for flexible regression modeling of high dimensional data the model takes the form of an expansion in product spline basis functions where the number of basis functions as well as the parameters associated with each one product degree and knot locations are automatically determined by the data this procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties unlike recursive partitioning however this method produces continuous models with continuous derivatives it has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables in addition the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions,ml_general,0.025907789626823968
1996,regression shrinkage and selection via the lasso,robert tibshirani,we propose a new method for estimation in linear models the lasso minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression it produces interpretable models like subset selection and exhibits the stability of ridge regression there is also an interesting relationship with recent work in adaptive function estimation by donoho and johnstone the lasso idea is quite general and can be applied in a variety of statistical models extensions to generalized regression models and tree based models are briefly described,ml_general,0.034245425857784006
1997,bias plus variance decomposition for zero one loss functions,ron kohavi and david h wolpert,we present a bias variance decomposition of expected misclassification rate the most commonly used loss function in supervised classification learning the bias variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms yet no decomposition was offered for the more commonly used zero one misclassification loss functions until the recent work of kong dietterich 1995 and breiman 1996 their decomposition suffers from some major shortcomings though eg potentially negative variance which our decomposition avoids we show that in practice the naive frequency based estimation of the decomposition terms is by itself biased and show how to correct for this bias we illustrate the decomposition on various algorithms and datasets from the uci repository,ml_general,0.02801000678101212
1958,the perceptron a probabilistic model for information storage and organization in the brain,rosenblatt f,to answer the questions of how information about the physical world is sensed in what form is information remembered and how does information retained in memory influence recognition and behavior a theory is developed for a hypothetical nervous system called a perceptron the theory serves as a bridge between biophysics and psychology it is possible to predict learning curves from neurological variables and vice versa the quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems,ml_general,0.02659083680108058
2011,deep sparse rectifier neural networks,xavier glorot et al,while logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons the latter work better for training multi layer neural networks this pa per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non linearity and non differentiable at zero creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data even though they can take advantage of semi supervised setups with extra unlabeled data deep rectifier net works can reach their best performance with out requiring any unsupervised pre training on purely supervised tasks with large labeled datasets hence these results can be seen as a new milestone in the attempts at under standing the difficulty in training deep but purely supervised neural networks and closing the performance gap between neural net works learnt with and without unsupervised pre training,ml_general,0.02665857619320237
1991,nonlinear principal component analysis using autoassociative neural networks,mark a kramer,nonlinear principal component analysis is a novel technique for multivariate data analysis similar to the well known method of principal component analysis nlpca like pca is used to identify and remove correlations among problem variables as an aid to dimensionality reduction visualization and exploratory data analysis while pca identifies only linear correlations between variables nlpca uncovers both linear and nonlinear correlations without restriction on the character of the nonlinearities present in the data nlpca operates by training a feedforward neural network to perform the identity mapping where the network inputs are reproduced at the output layer the network contains an internal bottleneck layer containing fewer nodes than input or output layers which forces the network to develop a compact representation of the input data and two additional hidden layers the nlpca method is demonstrated using time dependent simulated batch reaction data results show that nlpca successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters,ml_general,0.03501328460446993
